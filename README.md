# Next-Word-Prediction
Predicting what word comes next is one of the fundamental tasks of NLP and has several
applications. It is used by people every day, for example you while writing texts or emails. It
helps in minimizing keystrokes which in turn saves time while typing, checks spelling errors
and helps in word recall. Students who are still working on their language skills can take its
help. Also, people with dyslexia can be benefited by this. Also, it has become an integral part
in search algorithms like google search. According to a report by Markets and Markets [1],
“The global Natural Language Processing (NLP) market size to grow from USD 11.6 billion in
2020 to USD 35.1 billion by 2026, at a Compound Annual Growth Rate (CAGR) of 20.3% during
the forecast period.” The report also states, “The rise in the adoption of NLP-based
applications across verticals to enhance customer experience and increase in investments in
the healthcare vertical is expected to offer opportunities for NLP vendors”. This makes the
problem of word prediction interesting.
Machines can generate new text using various statistical, probabilistic, and complex
techniques called language models. These models are built to predict any probability of a
pattern or sequence of words. For this project we explored how different language model
architectures perform the job of text prediction the focus is on the n-gram language model,
Long Short-Term Memory (LSTM) neural networks, and (Generative Pre-trained Transformer)
GPT [2]. Language model. We used two different text corpus and have tried to understand
how these other models perform on them and identify if there are some tweaks that can be
made in the data or the model, that could make the performance better in the task of word
prediction. Also, the same text data was used to evaluate the models which gave insights into
how the models differ
